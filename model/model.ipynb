{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Masking, Embedding, SpatialDropout1D, Bidirectional\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import youtokentome as yttm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU')]\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices()\n",
    "print(physical_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental.set_memory_growth(physical_devices[2], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv').drop(columns=['uuid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.comment_text.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.comment_text.map(len) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('train.txt', 'w') as target:\n",
    "#     for row in train['comment_text']:\n",
    "#         target.write(str(row).strip() + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_model = yttm.BPE('bpe.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zdorovets_ai/miniconda3/envs/gpt/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "train['bpe'] = train.comment_text.apply(lambda row: np.array(bpe_model.encode(row, dropout_prob=0.1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = train.bpe.apply(lambda row: len(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "625.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ceil(np.mean(lens) + 4 * np.std(lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train.bpe.apply(lambda row: len(row)) <= 650]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data = tf.ragged.constant(train.bpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((train_data, train.toxicity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data.apply(tf.data.experimental.bucket_by_sequence_length(\n",
    "#                                 element_length_func=get_len, \n",
    "#                                 bucket_boundaries=[250], \n",
    "#                                 bucket_batch_sizes=[64, 500]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zdorovets_ai/miniconda3/envs/gpt/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "test['bpe'] = test.comment_text.apply(lambda row: bpe_model.encode(row, dropout_prob=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = test.bpe.apply(lambda row: len(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "603.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ceil(np.mean(lens) + 4 * np.std(lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[test.bpe.apply(lambda row: len(row)) <= 650]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = tf.ragged.constant(test.bpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = tf.data.Dataset.from_tensor_slices((test_data, test.toxicity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=10000,\n",
    "                    output_dim=256,\n",
    "                    embeddings_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.02, seed=None),\n",
    "                    trainable=True,\n",
    "                    mask_zero=True,\n",
    "                    name='Embeddings'))\n",
    "model.add(SpatialDropout1D(0.1, name='Emb_drop'))\n",
    "model.add(Bidirectional(LSTM(256,\n",
    "                             return_sequences=True,\n",
    "                             recurrent_dropout=0.5), \n",
    "                        name='Bi-LSTM_1'))\n",
    "# model.add(Dropout(0.25, name='Dropout_1'))\n",
    "model.add(Bidirectional(LSTM(256,\n",
    "                             recurrent_dropout=0.5), \n",
    "                        name='Bi-LSTM_2'))\n",
    "model.add(Dropout(0.25, name='Dropout_2'))\n",
    "model.add(Dense(512, name='Dense_1'))\n",
    "model.add(Dropout(0.5, name='Dropout_3'))\n",
    "model.add(Dense(256, name='Dense_2'))\n",
    "model.add(Dense(6, activation='relu', dtype='float32', name='Output'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Embeddings (Embedding)       (None, None, 256)         2560000   \n",
      "_________________________________________________________________\n",
      "Emb_drop (SpatialDropout1D)  (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "Bi-LSTM_1 (Bidirectional)    (None, None, 512)         1050624   \n",
      "_________________________________________________________________\n",
      "Bi-LSTM_2 (Bidirectional)    (None, 512)               1574912   \n",
      "_________________________________________________________________\n",
      "Dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "Dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 5,581,062\n",
      "Trainable params: 5,581,062\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-3)\n",
    "optimizer = mixed_precision.LossScaleOptimizer(optimizer, loss_scale='dynamic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(experimental_relax_shapes=True)\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(x, training=True)\n",
    "        loss = loss_object(y, predictions)\n",
    "        scaled_loss = optimizer.get_scaled_loss(loss)\n",
    "    scaled_gradients = tape.gradient(scaled_loss, model.trainable_variables)\n",
    "    gradients = optimizer.get_unscaled_gradients(scaled_gradients)\n",
    "    # gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in zip(scaled_gradients, model.trainable_variables)]\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    # optimizer.apply_gradients(gradients)\n",
    "    return loss, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(experimental_relax_shapes=True)\n",
    "def test_step(x):\n",
    "    predictions = model(x, training=False)\n",
    "    loss = loss_object(y, predictions)\n",
    "    return loss, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "854/854 [==============================] - 1351s 2s/step - loss: 0.1955 - acc: 0.9292\n",
      "214/214 [==============================] - 1682s 8s/step - loss: 0.2077 - acc: 0.9329\n"
     ]
    }
   ],
   "source": [
    "## Note: Rerunning this cell uses the same model variables\n",
    "\n",
    "# Keep results for plotting\n",
    "train_loss_results = []\n",
    "train_accuracy_results = []\n",
    "test_loss_results = []\n",
    "test_accuracy_results = []\n",
    "\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "metrics_names = ['loss','acc'] \n",
    "# Progress Bars\n",
    "# epochs_bar = tf.keras.utils.Progbar(EPOCHS)\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_bar = tf.keras.utils.Progbar(np.floor(int(tf.data.experimental.cardinality(train_data)/BATCH_SIZE))+1,\n",
    "                                           stateful_metrics=metrics_names)\n",
    "    test_bar = tf.keras.utils.Progbar(np.floor(int(tf.data.experimental.cardinality(test_data)/BATCH_SIZE))+1,\n",
    "                                          stateful_metrics=metrics_names)\n",
    "    train_loss_avg = tf.keras.metrics.Mean() # Avg loss\n",
    "    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy() \n",
    "    \n",
    "    test_loss_avg = tf.keras.metrics.Mean()\n",
    "    test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{EPOCHS}')\n",
    "\n",
    "    # Training loop - using batches of 32\n",
    "    for batch in train_data.shuffle(buffer_size=1024).batch(BATCH_SIZE): # Dataset (features, label)\n",
    "        X = batch[0].to_tensor() # RaggedTensor -> Sparse Tensor, Post Pad by the longest element\n",
    "        y = batch[1]\n",
    "        # Optimize the model\n",
    "        # loss_value, grads = grad(model, X, y) # compute loss and grad\n",
    "        # grads = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in zip(grads, model.trainable_variables)] # Clip\n",
    "        # optimizer.apply_gradients(grads) # Apply grad\n",
    "        loss, predictions = train_step(X, y)\n",
    "        \n",
    "        # Track progress\n",
    "        train_loss_avg.update_state(loss)  # Add current batch loss\n",
    "        \n",
    "        # Compare predicted label to actual label\n",
    "        # training=True is needed only if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "        train_accuracy.update_state(y, predictions)\n",
    "        values=[('loss', train_loss_avg.result()), ('acc', train_accuracy.result())]\n",
    "        train_bar.add(1, values=values)\n",
    "        \n",
    "    for batch in test_data.shuffle(buffer_size=1024, reshuffle_each_iteration=True).batch(BATCH_SIZE):\n",
    "        # Optimize the model\n",
    "        X = batch[0].to_tensor()\n",
    "        y = batch[1]\n",
    "        # loss_value = loss(model, X, y, training=False)\n",
    "        loss, predictions = train_step(X, y)\n",
    "        \n",
    "        # Track progress\n",
    "        test_loss_avg.update_state(loss)  # Add current batch loss\n",
    "        # Compare predicted label to actual label\n",
    "        # training=True is needed only if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "        test_accuracy.update_state(y, predictions)\n",
    "        values=[('loss', test_loss_avg.result()), ('acc', test_accuracy.result())]\n",
    "        test_bar.add(1, values=values)\n",
    "    \n",
    "    # End epoch\n",
    "    train_loss_results.append(train_loss_avg.result())\n",
    "    train_accuracy_results.append(train_accuracy.result())\n",
    "    test_loss_results.append(test_loss_avg.result())\n",
    "    test_accuracy_results.append(test_accuracy.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('toxicity_rnn_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "config = model.to_json()\n",
    "with open('config.json', 'w') as f:\n",
    "    json.dump(config, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
